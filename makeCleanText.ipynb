{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f25453e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T19:16:42.999929Z",
     "start_time": "2021-08-30T19:16:42.699441Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9615659d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T19:15:25.962664Z",
     "start_time": "2021-08-30T19:15:25.957098Z"
    }
   },
   "outputs": [],
   "source": [
    "def categoryToTarget(text):\n",
    "    if text == \"판매\":\n",
    "        return 1\n",
    "    elif text == \"교환\":\n",
    "        return 2\n",
    "    elif text == \"구입\":\n",
    "        return 3\n",
    "    elif text == \"거래완료\":\n",
    "        return 4\n",
    "    elif text == \"그냥드림\":\n",
    "        return 5\n",
    "    else:\n",
    "        return 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54bea155",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T19:15:30.508771Z",
     "start_time": "2021-08-30T19:15:26.929465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210701_clien\n",
      "20210702_clien\n",
      "20210703_clien\n",
      "20210704_clien\n",
      "20210705_clien\n"
     ]
    }
   ],
   "source": [
    "DATA_IN_PATH = './data/clienSample/'\n",
    "fileList = os.listdir(DATA_IN_PATH)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "# data = pd.DataFrame(columns=[\"title\",\"article\"])\n",
    "# targetData = pd.DataFrame(columns=[\"category\"])\n",
    "\n",
    "for fileName in fileList:\n",
    "    print(fileName)\n",
    "    if fileName == \".DS_Store\":\n",
    "        continue\n",
    "    with open(DATA_IN_PATH + fileName, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        \n",
    "\n",
    "    for document in json_data:\n",
    "        documentDataFrame = pd.DataFrame.from_dict([{\n",
    "            \"title\":document['title'],\n",
    "            \"article\":document['article'],\n",
    "            \"category\":document['category'],\n",
    "            \"category_to_target\":categoryToTarget(document['category']),\n",
    "            \"collectDate\":document['collectDate'],            \n",
    "            \"registerDate\":document['registerDate'],            \n",
    "        }])\n",
    "#         articleDataFrame = pd.DataFrame.from_dict([{\"title\":document['title'],\"article\":document['article']}])\n",
    "#         targetDataFrame = pd.DataFrame.from_dict([{\"category\":categoryToTarget(document['category'])}])\n",
    "\n",
    "        data = data.append(documentDataFrame)\n",
    "#         targetData = targetData.append(targetDataFrame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb05dc6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T19:16:50.344701Z",
     "start_time": "2021-08-30T19:16:44.916631Z"
    }
   },
   "outputs": [],
   "source": [
    "okt=Okt()\n",
    "article_text = okt.morphs(data.iloc[0]['article'])\n",
    "# print(article_text)\n",
    "stopWords = ['은', '는', '이', '가', '하', '아', '것', '들', '의', '있', '되', '수', '보', '주', '등', '한']\n",
    "\n",
    "def preprocessing(text, okt, remove_stopwords= False, stop_words=[]):\n",
    "    #줄바꿈 문자 삭제\n",
    "    text = text.replace(\"\\n\",\"\")\n",
    "    wordText = okt.morphs(text, stem=True)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        wordText = [token for token in wordText if not token in stop_words]\n",
    "        \n",
    "    return wordText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d10451d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T19:24:57.062440Z",
     "start_time": "2021-08-30T19:24:47.914665Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-887b9e5c17b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcleanTrainArticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mokt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopWords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcleanTrainArticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# preprocessing(data.iloc[3]['article'], okt, remove_stopwords=True, stop_words=stopWords)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "cleanTrainArticle = [preprocessing(article, okt, remove_stopwords=True, stop_words=stopWords) for article in data['article']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6b21212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T19:32:53.546582Z",
     "start_time": "2021-08-30T19:32:53.452737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[149, 37, 79, 2616, 393, 691, 340, 11, 2, 1, 97, 600, 195, 9, 1, 946, 175, 947, 691, 340, 4, 1, 3170, 120, 1, 1300, 10, 1301, 4, 1], [20, 1, 1195, 133, 885, 394, 7, 1625, 8, 332, 13, 601, 629, 6, 508, 2167, 1849, 1850, 44, 1625, 446, 1851, 6, 27, 2, 97, 257, 68, 1, 5, 42, 72, 73, 9, 748, 37, 36, 9, 748, 37, 60, 6, 1302, 33, 1303, 9, 749, 55, 2617, 68, 1852, 332, 42, 72, 35, 179, 172, 1, 35, 283, 24, 1, 17, 1, 1195, 1626, 20, 1627, 5, 13, 446, 1628, 27, 2, 1, 5, 1629, 434, 1630, 290, 42, 72, 1304, 55, 17, 1853, 948, 296, 36, 97, 257, 68, 1, 2618, 1305, 1429, 101, 259, 425, 9, 1, 354, 1854, 6, 11, 2, 1, 1306, 552, 12, 11, 107, 297, 1, 886, 17, 949, 3, 630, 20, 140, 1855, 1856, 26, 21, 19, 1, 14, 41, 364, 655, 298, 22, 435, 19, 1, 1300, 10, 1301], [110, 570, 20, 125, 11, 2, 1, 27, 52, 16, 509, 32, 11, 2, 37, 201, 4, 1857, 25, 77, 4538, 16, 2, 43, 1098, 25, 1631, 3171, 97, 32, 7, 35, 179, 8, 21, 3, 14, 2, 1], [207, 186, 17, 57, 5, 409, 6, 75, 11, 2, 244, 28, 124, 49, 87, 2, 602, 1632, 12, 67, 24], [1099, 119, 11, 2, 1, 629, 4539, 1196, 2, 31, 436, 51, 16, 2, 1, 4540, 33, 887, 2, 43, 95, 487, 4541, 1, 950, 631, 80, 9, 950, 16, 1858, 32, 1, 3172, 55, 4542, 109, 750, 50, 24, 1, 37, 395, 127, 42, 72, 149, 16, 4, 1, 284, 14, 12, 67, 24, 208, 46, 45, 4, 1, 355, 39, 34, 1]]\n",
      "7591\n"
     ]
    }
   ],
   "source": [
    "len(cleanTrainArticle)\n",
    "\n",
    "cleanTrainDF = pd.DataFrame({'article':cleanTrainArticle, 'category':data['category_to_target']})\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(cleanTrainArticle)\n",
    "text_sequences = tokenizer.texts_to_sequences(cleanTrainArticle)\n",
    "\n",
    "print(text_sequences[0:5])\n",
    "wordVocab = tokenizer.word_index\n",
    "wordVocab[\"<PAD>\"] = 0\n",
    "# print(wordVocab)\n",
    "print(len(wordVocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ded863",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = {}\n",
    "data_configs['vocab'] = word_vocab\n",
    "data_configs['vocab_size'] = len(wordVocab)+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
